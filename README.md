[Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/)ä»¥åŠå…¶åç»­æ–‡ç« 

[A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems](http://blog.csdn.net/shine19930820/article/details/78810984)çš„å®ç°Demoã€‚

# æ³¨æ„ï¼š
**\*\*\*\*2020/11/15\*\*\*\***

è®ºæ–‡[li2020sentence](https://arxiv.org/abs/2011.05864)å°†normalizing flowså’Œbertç»“åˆï¼Œåœ¨è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡ä¸Šæœ‰å¥‡æ•ˆï¼Œæ¥ä¸‹æ¥ä¼šç»§ç»­è¿›è¡ŒéªŒè¯ã€‚

**\*\*\*\*2020/10/27\*\*\*\***

æ·»åŠ åº•å±‚ä½¿ç”¨bertçš„siamese-bertå®éªŒï¼Œè§[siamese\_network.py](https://github.com/InsaneLife/dssm/blob/master/model/siamese_network.py)ä¸­ç±» SiamenseBertï¼Œå…¶ä»–å’Œä¸‹é¢ä¸€æ ·.

ç›¸æ¯”äº[bert](https://github.com/InsaneLife/dssm/blob/master/model/bert_classifier.py) ç›´æ¥å°†ä¸¤å¥ä½œä¸ºè¾“å…¥ï¼ŒåŒå¡”bertçš„ä¼˜åŠ¿åœ¨äºï¼š
- max sequence lenä¼šæ›´çŸ­ï¼Œè®­ç»ƒæ‰€éœ€è¦æ˜¾å­˜æ›´å°ï¼Œé€Ÿåº¦ä¼šç¨å¾®å¿«ä¸€äº›ï¼Œå¯¹äºç¡¬ä»¶ä¸æ˜¯å¤ªå¥½çš„ä¼™ä¼´æ¯”è¾ƒå‹å¥½ã€‚
- å¯ä»¥è®­ç»ƒåä½¿ç”¨bertä½œä¸ºå¥å­embeddingçš„encoderï¼Œåœ¨ä¸€äº›çº¿ä¸ŠåŒ¹é…çš„æ—¶å€™ï¼Œå¯ä»¥é¢„å…ˆå°†éœ€è¦å¯¹æ¯”çš„å¥å­å‘é‡ç®—å‡ºæ¥ï¼ŒèŠ‚çœå®æ—¶ç®—åŠ›ã€‚
- æ•ˆæœç›¸æ¯”äºç›´æ¥ç”¨bertè¾“å…¥ä¸¤å¥ï¼Œæµ‹è¯•é›†ä¼šå·®ä¸€ä¸ªå¤šç‚¹ã€‚
- bertå¯ä»¥ä½¿ç”¨[CLS]çš„è¾“å‡ºæˆ–è€…average context embedding, ä¸€èˆ¬åè€…æ•ˆæœä¼šæ›´å¥½ã€‚
```shell
# bert_siameseåŒå¡”æ¨¡å‹
python train.py --mode=train --method=bert_siamese
# ç›´æ¥ä½¿ç”¨åŠŸèƒ½bert
python train.py --mode=train --method=bert
```
å‚è€ƒï¼š[reimers2019sentence](https://arxiv.org/abs/1908.10084)

**\*\*\*\*2020/10/17\*\*\*\***

ç”±äºä¹‹å‰æ•°æ®é›†é—®é¢˜ï¼Œä¼šæœ‰ä¸æ”¶æ•›é—®é¢˜ï¼Œç°æ›´æ¢æ•°æ®é›†ä¸ºLCQMCå£è¯­åŒ–æè¿°çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ•°æ®é›†ã€‚æ¨¡å‹ä¹Ÿä»å¤šå¡”å˜æˆäº†åŒå¡”æ¨¡å‹ï¼Œè§[siamese\_network.py](https://github.com/InsaneLife/dssm/blob/master/model/siamese_network.py), è®­ç»ƒå…¥å£ï¼š[train.py](https://github.com/InsaneLife/dssm/blob/master/train.py)

> éš¾ä»¥æ‰¾åˆ°æœç´¢ç‚¹å‡»çš„å…¬å¼€æ•°æ®é›†ï¼Œæš‚ä¸”ç”¨è¯­ä¹‰ç›¸ä¼¼ä»»åŠ¡æ•°æ®é›†ï¼Œæœ‰ç‚¹å˜å‘³äº†ï¼Œå“ˆå“ˆ
> ç›®å‰çœ‹åœ¨æ­¤æ•°æ®é›†ä¸Šæµ‹è¯•æ•°æ®é›†çš„å‡†ç¡®ç‡æ˜¯æå‡çš„ï¼Œåªæœ‰ä¸ƒåå¤šï¼Œä½†æ˜¯è¦è¾¾åˆ°è®ºæ–‡çš„å‡†ç¡®ç‡ï¼Œä»ç„¶è¿˜éœ€è¦è¿›è¡Œè°ƒå‚

è®­ç»ƒï¼ˆé»˜è®¤ä½¿ç”¨åŠŸLCQMCæ•°æ®é›†ï¼‰ï¼š

```shell
python train.py --mode=train
```

é¢„æµ‹ï¼š

```shell
python train.py --mode=train --file=$predict_file$
```

æµ‹è¯•æ–‡ä»¶æ ¼å¼: q1\tq2, ä¾‹å¦‚ï¼š

```
ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·	ä»Šå¤©æ¸©åº¦æ€ä¹ˆæ ·
```



**\*\*\*\*2019/5/18\*\*\*\***

ç”±äºä¹‹å‰ä»£ç apiè¿‡æ—¶ï¼Œå·²æ›´æ–°æœ€æ–°ä»£ç äºï¼š[dssm\_rnn.py](https://github.com/InsaneLife/dssm/blob/master/dssm_rnn.py)

æ•°æ®å¤„ç†ä»£ç [data\_input.py](https://github.com/InsaneLife/dssm/blob/master/data_input.py) å’Œæ•°æ®[data](https://github.com/InsaneLife/dssm/tree/master/data) å·²ç»æ›´æ–°ï¼Œç”±äºä½¿ç”¨äº†rnnï¼Œæ‰€ä»¥**è¾“å…¥ébag of wordsæ–¹å¼ã€‚**

![img](https://ask.qcloudimg.com/http-save/yehe-1881084/7ficv1hhqf.png?imageView2/2/w/1620)

> æ¥æºï¼šPalangi, Hamid, et al. "Semantic modelling with long-short-term memory for information retrieval." arXiv preprint arXiv:1412.6629 2014.
> 
> è®­ç»ƒæŸå¤±ï¼Œåœ¨45ä¸ªepochæ—¶åŸºæœ¬ä¸ä¸‹é™ï¼š
> 
> ![dssm_rnn_loss](https://raw.githubusercontent.com/InsaneLife/dssm/master/assets/dssm_rnn_loss.png)

# 1\. æ•°æ®&ç¯å¢ƒ

DSSMï¼Œå¯¹äºè¾“å…¥æ•°æ®æ˜¯Queryå¯¹ï¼Œå³QueryçŸ­å¥å’Œç›¸åº”çš„å±•ç¤ºï¼Œå±•ç¤ºä¸­åˆ†ç‚¹å‡»å’Œæœªç‚¹å‡»ï¼Œåˆ†åˆ«ä¸ºæ­£è´Ÿæ ·ï¼ŒåŒæ—¶å¯¹äºç‚¹å‡»çš„å…ˆåé¡ºåºï¼Œä¹Ÿæ˜¯æœ‰ä¸åŒèµ‹å€¼ï¼Œå…·ä½“å¯å‚è€ƒè®ºæ–‡ã€‚

å¯¹äºæˆ‘çš„Queryæ•°æ®æœ¬äººæ— æƒå¼€æ”¾ï¼Œè¿˜è¯·è‡ªè¡Œå¯»æ‰¾æ•°æ®ã€‚
ç¯å¢ƒï¼š

1. win, python3.5, tensorflow1.4.

# 2\. word hashing

åŸæ–‡ä½¿ç”¨3-gramsï¼Œå¯¹äºä¸­æ–‡ï¼Œæˆ‘ä½¿ç”¨äº†uni-gramï¼Œå› ä¸ºä¸­æ–‡æœ¬èº«å­—æœ‰ä¸€å®šä»£è¡¨æ„ä¹‰ï¼ˆä¹Ÿæœ‰è®ºæ–‡æ‹†ç¬”ç”»ï¼‰ï¼Œå¯¹äºæ¯ä¸ªgraméƒ½ä½¿ç”¨one-hotç¼–ç ä»£æ›¿ï¼Œæœ€ç»ˆå¯ä»¥å¤§å¤§é™ä½çŸ­å¥ç»´åº¦ã€‚

# 3\. ç»“æ„

ç»“æ„å›¾ï¼š

![img](https://raw.githubusercontent.com/InsaneLife/MyPicture/master/dssm2.png)

1. æŠŠæ¡ç›®æ˜ å°„æˆä½ç»´å‘é‡ã€‚
2. è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£çš„cosineç›¸ä¼¼åº¦ã€‚

## 3.1 è¾“å…¥

è¿™é‡Œä½¿ç”¨äº†TensorBoardå¯è§†åŒ–ï¼Œæ‰€ä»¥å®šä¹‰äº†name\_scope:

``` python
with tf.name_scope('input'):
    query_batch = tf.sparse_placeholder(tf.float32, shape=[None, TRIGRAM_D], name='QueryBatch')
    doc_positive_batch = tf.sparse_placeholder(tf.float32, shape=[None, TRIGRAM_D], name='DocBatch')
    doc_negative_batch = tf.sparse_placeholder(tf.float32, shape=[None, TRIGRAM_D], name='DocBatch')
    on_train = tf.placeholder(tf.bool)
```

## 3.2 å…¨è¿æ¥å±‚

æˆ‘ä½¿ç”¨ä¸‰å±‚çš„å…¨è¿æ¥å±‚ï¼Œå¯¹äºæ¯ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œé™¤äº†ç¥ç»å…ƒä¸ä¸€æ ·ï¼Œå…¶ä»–éƒ½ä¸€æ ·ï¼Œæ‰€ä»¥å¯ä»¥å†™ä¸€ä¸ªå‡½æ•°å¤ç”¨ã€‚
$$
l\_n = W\_n x + b\_1
$$

``` python
def add_layer(inputs, in_size, out_size, activation_function=None):
    wlimit = np.sqrt(6.0 / (in_size + out_size))
    Weights = tf.Variable(tf.random_uniform([in_size, out_size], -wlimit, wlimit))
    biases = tf.Variable(tf.random_uniform([out_size], -wlimit, wlimit))
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    return outputs
```

å…¶ä¸­ï¼Œå¯¹äºæƒé‡å’ŒBiasï¼Œä½¿ç”¨äº†æŒ‰ç…§è®ºæ–‡çš„ç‰¹å®šçš„åˆå§‹åŒ–æ–¹å¼ï¼š

``` python
	wlimit = np.sqrt(6.0 / (in_size + out_size))
    Weights = tf.Variable(tf.random_uniform([in_size, out_size], -wlimit, wlimit))
    biases = tf.Variable(tf.random_uniform([out_size], -wlimit, wlimit))
```

### Batch Normalization

``` python
def batch_normalization(x, phase_train, out_size):
    """
    Batch normalization on convolutional maps.
    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow
    Args:
        x:           Tensor, 4D BHWD input maps
        out_size:       integer, depth of input maps
        phase_train: boolean tf.Varialbe, true indicates training phase
        scope:       string, variable scope
    Return:
        normed:      batch-normalized maps
    """
    with tf.variable_scope('bn'):
        beta = tf.Variable(tf.constant(0.0, shape=[out_size]),
                           name='beta', trainable=True)
        gamma = tf.Variable(tf.constant(1.0, shape=[out_size]),
                            name='gamma', trainable=True)
        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')
        ema = tf.train.ExponentialMovingAverage(decay=0.5)

        def mean_var_with_update():
            ema_apply_op = ema.apply([batch_mean, batch_var])
            with tf.control_dependencies([ema_apply_op]):
                return tf.identity(batch_mean), tf.identity(batch_var)

        mean, var = tf.cond(phase_train,
                            mean_var_with_update,
                            lambda: (ema.average(batch_mean), ema.average(batch_var)))
        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)
    return normed
```

### å•å±‚

``` python
with tf.name_scope('FC1'):
    # æ¿€æ´»å‡½æ•°åœ¨BNä¹‹åï¼Œæ‰€ä»¥æ­¤å¤„ä¸ºNone
    query_l1 = add_layer(query_batch, TRIGRAM_D, L1_N, activation_function=None)
    doc_positive_l1 = add_layer(doc_positive_batch, TRIGRAM_D, L1_N, activation_function=None)
    doc_negative_l1 = add_layer(doc_negative_batch, TRIGRAM_D, L1_N, activation_function=None)

with tf.name_scope('BN1'):
    query_l1 = batch_normalization(query_l1, on_train, L1_N)
    doc_l1 = batch_normalization(tf.concat([doc_positive_l1, doc_negative_l1], axis=0), on_train, L1_N)
    doc_positive_l1 = tf.slice(doc_l1, [0, 0], [query_BS, -1])
    doc_negative_l1 = tf.slice(doc_l1, [query_BS, 0], [-1, -1])
    query_l1_out = tf.nn.relu(query_l1)
    doc_positive_l1_out = tf.nn.relu(doc_positive_l1)
    doc_negative_l1_out = tf.nn.relu(doc_negative_l1)
Â·Â·Â·Â·Â·Â·
```

åˆå¹¶è´Ÿæ ·æœ¬

``` python
with tf.name_scope('Merge_Negative_Doc'):
    # åˆå¹¶è´Ÿæ ·æœ¬ï¼Œtileå¯é€‰æ‹©æ˜¯å¦æ‰©å±•è´Ÿæ ·æœ¬ã€‚
    doc_y = tf.tile(doc_positive_y, [1, 1])
    for i in range(NEG):
        for j in range(query_BS):
            # slice(input_, begin, size)åˆ‡ç‰‡API
            doc_y = tf.concat([doc_y, tf.slice(doc_negative_y, [j * NEG + i, 0], [1, -1])], 0)
```

## 3.3 è®¡ç®—cosç›¸ä¼¼åº¦

``` python
with tf.name_scope('Cosine_Similarity'):
    # Cosine similarity
    # query_norm = sqrt(sum(each x^2))
    query_norm = tf.tile(tf.sqrt(tf.reduce_sum(tf.square(query_y), 1, True)), [NEG + 1, 1])
    # doc_norm = sqrt(sum(each x^2))
    doc_norm = tf.sqrt(tf.reduce_sum(tf.square(doc_y), 1, True))

    prod = tf.reduce_sum(tf.multiply(tf.tile(query_y, [NEG + 1, 1]), doc_y), 1, True)
    norm_prod = tf.multiply(query_norm, doc_norm)

    # cos_sim_raw = query * doc / (||query|| * ||doc||)
    cos_sim_raw = tf.truediv(prod, norm_prod)
    # gamma = 20
    cos_sim = tf.transpose(tf.reshape(tf.transpose(cos_sim_raw), [NEG + 1, query_BS])) * 20
```

## 3.4 å®šä¹‰æŸå¤±å‡½æ•°

``` python
with tf.name_scope('Loss'):
    # Train Loss
    # è½¬åŒ–ä¸ºsoftmaxæ¦‚ç‡çŸ©é˜µã€‚
    prob = tf.nn.softmax(cos_sim)
    # åªå–ç¬¬ä¸€åˆ—ï¼Œå³æ­£æ ·æœ¬åˆ—æ¦‚ç‡ã€‚
    hit_prob = tf.slice(prob, [0, 0], [-1, 1])
    loss = -tf.reduce_sum(tf.log(hit_prob))
    tf.summary.scalar('loss', loss)
```

## 3.5é€‰æ‹©ä¼˜åŒ–æ–¹æ³•

``` python
with tf.name_scope('Training'):
    # Optimizer
    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(loss)
```

## 3.6 å¼€å§‹è®­ç»ƒ

``` python
# åˆ›å»ºä¸€ä¸ªSaverå¯¹è±¡ï¼Œé€‰æ‹©æ€§ä¿å­˜å˜é‡æˆ–è€…æ¨¡å‹ã€‚
saver = tf.train.Saver()
# with tf.Session(config=config) as sess:
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train', sess.graph)
    start = time.time()
    for step in range(FLAGS.max_steps):
        batch_id = step % FLAGS.epoch_steps
        sess.run(train_step, feed_dict=feed_dict(True, True, batch_id % FLAGS.pack_size, 0.5))
```

GitHubå®Œæ•´ä»£ç  [https://github.com/InsaneLife/dssm](https://github.com/InsaneLife/dssm)

Multi-view DSSMå®ç°åŒç†ï¼Œå¯ä»¥å‚è€ƒGitHubï¼š[multi\_view\_dssm\_v3](https://github.com/InsaneLife/dssm/blob/master/multi_view_dssm_v3.py)

CSDNåŸæ–‡ï¼š[http://blog.csdn.net/shine19930820/article/details/79042567](http://blog.csdn.net/shine19930820/article/details/79042567)

## è‡ªåŠ¨è°ƒå‚
å‚æ•°æœç´¢ç©ºé—´ï¼š[search_space.json](./configs/search_space.json)
é…ç½®æ–‡ä»¶ï¼š[auto_ml.yml](auto_ml.yml)
å¯åŠ¨å‘½ä»¤
```shell
nnictl create --config auto_ml.yml -p 8888
```
> ç”±äºæ²¡æœ‰gpu ğŸ˜‚ï¼Œ[auto_ml.yml](auto_ml.yml)è®¾ç½®ä¸­æ²¡æœ‰é…ç½®gpuï¼Œæœ‰gpuåŒå­¦å¯è‡ªè¡Œé…ç½®ã€‚

è¯¦ç»†æ–‡æ¡£ï¼šhttps://nni.readthedocs.io/zh/latest/Overview.html


# Reference
- [li2020sentence](https://arxiv.org/abs/2011.05864)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- nni è°ƒå‚: https://nni.readthedocs.io/zh/latest/Overview.html